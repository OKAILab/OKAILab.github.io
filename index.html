<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>OKAILab</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="assets/iccv-navbar-logo.svg" alt="ICCV Logo" style="position: absolute; left: 1em; max-height: 40px;">
      <a href="#">Home</a>
      <a href="#intro">About</a>
      <a href="#Topics">Topics</a>
      <a href="#Members">Members</a>
    </div>
  </div>

  <div class="title-container">
    <div class="overlay"></div>
    <div class="content" style="text-align: center; margin: 20px">
      <h1>OKAILab</h1>
      <div class="subtitle">
        ShanghaiTech University
      </div>
    </div>
  </div>

  <div class="container">
    <div class="section" id="intro">
      <h2>About</h2>

      <p>
        Learning Group focus on Spatial Intelligence and Embodied AI.

<!--        <ul>-->
<!--          <li>Transferring knowledge from human-human and human-scene interaction and collaboration to inform the development of humanoids and other embodied agents (e.g., via retargeting).</li>-->
<!--          <li>Exploring different methods for deriving visual representations that capture object properties, dynamics, and affordances relevant to human-robot collaboration.</li>-->
<!--          <li>Investigating methods for modeling and predicting human intentions to enable robots to anticipate actions and respond safely.</li>-->
<!--          <li>Integrating robots into interactive settings to foster seamless and effective teamwork.</li>-->
<!--          <li>Establishing meaningful benchmarks and metrics to measure advancements in human-robot-scene interaction and collaboration.</li>-->
<!--        </ul>-->
      </p>
    </div>

    <div class="section" id="call">
      <h2>Topics</h2>
      <p>
        We invite submissions of <strong>long papers</strong> (up to 8 pages excluding references) and <strong>short papers</strong> (up to 4 pages excluding references) that explore human-robot-scene interaction and collaboration.
      </p>

      <h3>Large Language Models</h3>
        <ul>
            <li>LLM,MLLM,LMM,VLM,etc</li>
            <li>Reasoning</li>
            <li>Affordance</li>
            <li>Appications</li>
        </ul>

        <h3>3D Computer Vision</h3>
        <ul>
            <li>Human-OI:Human-Object interaction</li>
            <li>Hand-OI:Hand-Object interaction</li>
            <li>Affordance</li>

        </ul>

        <h3>Robot Learning</h3>
        <ul>
            <li>Robot Grasp</li>
            <li>Humanoid Locomotion</li>
            <li>Reinforcement Learning</li>
            <li>Robot Manipulation</li>
        </ul>
    </div>

    <div class="section" id="speakers">
      <h2>Members</h2>
      <p> listed alphabetically </p>
      <div class="people">
        <a href="https://shuangli59.github.io/">
          <img src="assets/shuangli-photo.jpeg">
          <div>Shuang Li</div>
          <div class="affiliation">Stanford University</div>
        </a>
        <a href="https://roozbehm.info/">
          <img src="assets/profile_roozbehM.jpg">
          <div>Roozbeh Mottaghi</div>
          <div class="affiliation">Meta AI</div>
        </a>
        <a href="https://www.iit.it/people-details/-/people/lorenzo-natale">
          <img src="assets/lorenzo_natale.jpeg">
          <div>Lorenzo Natale</div>
          <div class="affiliation">Italian Institute of Technology</div>
        </a>
        <a href="https://www.lerrelpinto.com/">
          <img src="assets/lerrel_pinto.jpg">
          <div>Lerrel Pinto</div>
          <div class="affiliation">New York University</div>
        </a>
        <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">
          <img src="assets/gpm.png">
          <div>Gerard Pons-Moll</div>
          <div class="affiliation">University of TÃ¼bingen</div>
        </a>
      </div>
    </div>



  <footer>
    <p>&copy; OKAILab</p>
  </footer>
</body>

</html>